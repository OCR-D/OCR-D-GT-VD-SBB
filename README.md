# Title
OCR-D-GT-VD-SBB

# Description
A ground truth (GT) dataset created within the OCR-D project and consisting of 348 pages extracted from historical documents pertaining to the "Verzeichnis der im deutschen Sprachraum erschienenen Drucke" (VD), all of which have been digitised by Staatsbibliothek zu Berlin – Berlin State Library (SBB). The data publication consists of 348 .xml files with transcriptions for  348 .tif facsimile image files. The image files pertain to 67 distinct works; four images were extracted from each of the 65 works; from two further works, 49 and 39 images respectively were extracted to create the GT. The dataset is complemented by a .csv file which contains a mapping between the identifiers used in this dataset and the unique identifiers used in the digitised collections of Staatsbibliothek zu Berlin – Berlin State Library, as well as a filelisting in .csv format. Data selection was performed within the [OCR-D](http://ocr-d.de) project at Staatsbibliothek zu Berlin – Berlin State Library. The project is funded by the German Research Foundation DFG, project grant no. 460675868. GT data were established by a digitisation service provider and post-corrected by staff members of the Berlin State Library, data curation and publication was done by two members of the team of the research project "[Human.Machine.Culture](https://mmk.sbb.berlin/?lang=en)" at Staatsbibliothek zu Berlin – Berlin State Library. The research project was funded by the Federal Government Commissioner for Culture and the Media (BKM), project grant no. 2522DIG002.

## Homepage
[https://ocr-d.de/](https://ocr-d.de/ )

## Publisher
Staatsbibliothek zu Berlin – Berlin State Library

## Dataset Curators
Data selection and quality assessment was performed by three members of the OCR-D project:

Clemens Neudecker, Staatsbibliothek zu Berlin – Berlin State Library, [Clemens.Neudecker@sbb.spk-berlin.de](mailto:Clemens.Neudecker@sbb.spk-berlin.de), ORCID: [0000-0001-5293-8322](https://orcid.org/0000-0001-5293-8322). Clemens Neudecker has studied philosophy, computer science and political science. He was responsible for data selection and the quality assessment of the dataset.

Konstantin Baierer, Staatsbibliothek zu Berlin – Berlin State Library, [Konstantin.Baierer@sbb.spk-berlin.de](hmailto:Konstantin.Baierer@sbb.spk-berlin.de), ORCID: [0000-0003-2397-242X](https://orcid.org/0000-0003-2397-242X). Konstantin studied library and information science and computer science. He works on the OCR-D project and was responsible for the software used in repairing the data received by the digitisation service provider.

Maria Federbusch, Staatsbibliothek zu Berlin - Berlin State Library,
[Maria.Federbusch@sbb.spk-berlin.de](mailto:Maria.Federbusch@sbb.spk-berlin.de)

The dataset was curated and published by two members of the research project "Mensch.Maschine.Kultur" ("Human.Machine.Culture"), using GT data established by the digitisation service provider.

Mike Gerber, Staatsbibliothek zu Berlin – Berlin State Library, [Mike.Gerber@sbb.spk-berlin.de](mailto:Mike.Gerber@sbb.spk-berlin.de), ORCID: [0000-0003-2167-4322](https://orcid.org/0000-0003-2167-4322). Mike Gerber has studied computer science and worked in the research project "Mensch.Maschine.Kultur"; he was responsible for processing the data and curating them as described below.

Jörg Lehmann, Staatsbibliothek zu Berlin – Berlin State Library, [Joerg.Lehmann@sbb.spk-berlin.de](mailto:Joerg.Lehmann@sbb.spk-berlin.de), ORCID: [0000-0003-1334-9693](https://orcid.org/0000-0003-1334-9693). Jörg Lehmann has studied history and comparative literature and works in the research project "Mensch.Maschine.Kultur"; he was responsible for correcting regions and their labels in the GT data and for drafting the datasheet.


## Other Contributors
GT data were initially produced by an external digitisation service provider.

## Point of Contact
Clemens Neudecker, Staatsbibliothek zu Berlin – Berlin State Library, [Clemens.Neudecker@sbb.spk-berlin.de](mailto:Clemens.Neudecker@sbb.spk-berlin.de), ORCID: [0000-0001-5293-8322](https://orcid.org/0000-0001-5293-8322). 

Konstantin Baierer, Staatsbibliothek zu Berlin – Berlin State Library, [Konstantin.Baierer@sbb.spk-berlin.de](hmailto:Konstantin.Baierer@sbb.spk-berlin.de), ORCID: [0000-0003-2397-242X](https://orcid.org/0000-0003-2397-242X).

## Papers and/or Other References
Michał Bubula, Konstantin Baierer, Jörg Lehmann, Clemens Neudecker, Vahid Rezanezhad, Doris Škarić, "How Scalable is Quality Assessment of Text Recognition? A Combination of Ground Truth and Confidence Scores." In: Proceedings of the 6th Computational Humanities Research Conference, ed. by Taylor Arnold, Margherita Fantoli, and Ruben Ros. Vol. 1. Anthology of Computers and the Humanities. 2025, 1–25. At the time of the publication of the dataset, no DOI was available for this publication.

## Supported Tasks and Shared Tasks
The dataset was not part of a shared task.

### AI Category 
[Natural Language Processing](https://ai4culture.eu/resources?page=0&aiCategories=NATURAL_LANGUAGE_PROCESSING&resourceType=all), [Feature Extraction](https://huggingface.co/tasks/feature-extraction)

### Type of Cultural Heritage Application
[Digitisation](https://ai4culture.eu/resources?page=0&chApplications=DIGITISATION&resourceType=all)

### (Cultural Heritage) Application Example
Optical Character Recognition (OCR)

# Distribution
This dataset is distributed by the named dataset curators. Dissemination on the publication was done via a paper publication and social media in order to reach the relevant research and machine learning communities.

## Data Access URL
[https://doi.org/10.5281/zenodo.17395956](https://doi.org/10.5281/zenodo.17395956 ) 

## Licensing Information
[Creative Commons Attribution 4.0 International – CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode)

## File Format
text/csv (utf-8), text/xml (utf-8), image/tif

## Citation Information
```
@dataset{baierer_2025_17395956,
  author       = {Baierer, Konstantin and
                  Federbusch, Maria and
                  Gerber, Mike and
                  Lehmann, Jörg and
                  Neudecker, Clemens},
  title        = {OCR-D-GT-VD-SBB},
  month        = oct,
  year         = 2025,
  publisher    = {Staatsbibliothek zu Berlin – Berlin State Library},
  version      = 1,
  doi          = {10.5281/zenodo.17395956},
  url          = {https://doi.org/10.5281/zenodo.17395956},
}

```

# Composition
In Germany, three separate cooperative projects exist to catalogue and digitise all prints for the sixteenth, seventeenth and eighteenth century published in the then German-speaking countries. The full title of these three projects are: "Verzeichnis der im deutschen Sprachbereich erschienenen Drucke des 16. Jahrhunderts" (in English: Register of printed works of the 16th century published in German-speaking countries, abbreviated [VD16](http://www.vd16.de/)); "Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts" (in English: Union catalogue of books printed in German-speaking countries in the 17th century, abbreviated [VD17](http://www.vd17.de/), also available as a database [VD17](https://kxp.k10plus.de/DB=1.28/)); and "Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 18. Jahrhunderts" (in English: Union catalogue of books printed in German-speaking countries in the 18th century, abbreviated [VD18](http://www.vd18.de/), also available as a database [VD18](https://vd18.k10plus.de/)). All three projects are funded by the [Deutsche Forschungsgemeinschaft](https://www.dfg.de/en) (German Research Foundation). Included are all German-language items as well as any work printed and published in the German-speaking countries of the time, regardless of its language.

Staatsbibliothek zu Berlin – Berlin State Library has digitised several thousand works within the three distinct VD projects. From these digitised works, 67 were chosen that were published between 1509 and 1827. The selection was made with support from curators of the manuscripts and historical prints department of the Berlin State Library with the intention to both cover as widely as possible the different document types contained in the vast VD collection and also considering features exhibited by those documents that have shown to be problematic for OCR processing. While 67 works formed the basis of the whole dataset, 4 images were extracted from 65 works at a time; from the two further works, 49 and 39 images respectively were extracted to create the GT. Consequently, the dataset consists of 348 image files as well as 348 PAGE-XML files. Moreover, a mapping between the identifiers used in this dataset and the unique identifiers used in the digitised collections of Staatsbibliothek zu Berlin – Berlin State Library is enclosed, as well as a filelisting in .csv format.


## Data Category 
content

## Media Category
image and text

## Object Type
[monographs](http://vocab.getty.edu/page/aat/300060417); [funeral books](http://vocab.getty.edu/page/aat/300263078); [dissertations](http://vocab.getty.edu/page/aat/300028029); [broadsheet (format)](http://vocab.getty.edu/page/aat/300310088); [rdaco:1020](http://rdaregistry.info/termList/RDAContentType/1020) "text";


## Dataset Structure
The 348 pages of GT are organised in 67 directories, each one for a single work; each of these directories contain a single xml file – each one for a single work – as well as a subdirectory which is always named OCR-D-IMG. In the latter subdirectories, the image files can be found in .tif format.

### Data Instances
Not applicable.

### Data Fields
Not applicable.

### Compliance with Standard(s)
The xml files containing the GT data conform to the PAGE-XML schema developed by PRImA Research, see [https://ocr-d.de/en/gt-guidelines/trans/trPage.html](https://ocr-d.de/en/gt-guidelines/trans/trPage.html). PAGE stands for **P**age **A**nalysis and **G**round truth **E**lements, whereas PRImA stands for the **P**attern **R**ecognition & **Im**age **A**nalysis Lab at the University of Salford, Manchester.

### Data Splits
This dataset does not contain data splits which could be used for machine learning tasks. It is advisable to first analyse the dataset and then perform the split(s).

## Languages
The languages represented in the dataset (i.e. the language of the publications in which the individual works were published) are, in descending order of their frequency, German (ger, 40 works), Latin (lat, 23 works), French (fre, 2 works), Latin and German (lat + ger, 1 work, [PPN85859577X](https://digital.staatsbibliothek-berlin.de/werkansicht?PPN=PPN85859577X)), and low German (nds, 1 work, [PPN82562360X](https://digital.staatsbibliothek-berlin.de/werkansicht?PPN=PPN82562360X)). As this language description relates to the main body of the text, the language used on the pages selected for establishing GT may differ. The languages of the publications are noted in [ISO 639-2/B](https://www.loc.gov/standards/iso639-2/php/code_list.php) format.

## Descriptive Statistics
The 67 selected works were published between 1509 and 1827 and in 41 publication places. The most common amongst them are Nürnberg (7), Leipzig (6), Frankfurt am Main (4), but also Amsterdam, Bern, and Colmar with each one published there respectively. This is in line with the definition of works pertaining to the "Verzeichnis der im deutschen Sprachraum erschienenen Drucke" (VD). Thematically, these works pertain to the topics theology, languages and literatures, jurisprudence, history, ethnography, geography, sciences and mathematics, medicine, to name but the most often classifications. 30 of these works pertain to the VD16, 5 to the VD17, and 31 to the VD18. One work (PPN796229600, published 1827) was not sorted into one of the VD projects. All works were published under a Public Domain Mark 1.0 licence. However, while these statistics tell something about the works themselves, it has to be underlined here that this publication of GT data serves in the first line the task of optical character recognition. The more interesting information is therefore the number of GT pages (348) as well as the number of 486.098 characters.

# Data Collection Process
The idea behind the decision to select image files from the VD dataset at Staatsbibliothek zu Berlin – Berlin State Library was to compile a reasonably large GT dataset suitable to serve the needs specific for the mostly German and Latin texts published in the three VD projects.

## Curation Rationale
The curation rationale was driven by the intention to provide a GT dataset.

## Source Data


### Initial Data Collection
Initial data collection was performed by one of the three VD projects. For more information, see the [VD16](http://www.vd16.de/)), [VD17](http://www.vd17.de/), and [VD18](http://www.vd18.de/) websites.


### Source Data Producers
The source data were produced by a digitisation service provider in PAGE-XML format. To produce GT data, the transcription guidelines of the OCR-D project on level 3 were followed; they are documented at [http://www.ocr-d.de/sites/all/gt_guidelines/index.html](http://www.ocr-d.de/sites/all/gt_guidelines/index.html). According to these guidelines, the accuracy of data capture is 99.95% at character level, i.e. out of 10,000 characters captured, only a maximum of 5 characters may be incorrectly captured. Data quality was checked and approved by Staatsbibliothek zu Berlin – Berlin State Library.

### Digitisation Pipeline
Not applicable.

## Preprocessing and Cleaning
GT data established by the digitisation service provider were checked for accuracy; inconsistencies were detected and repaired using a script provided by the OCR-D project; it is available at [https://github.com/qurator-spk/ocrd_repair_inconsistencies/blob/master/README.md](https://github.com/qurator-spk/ocrd_repair_inconsistencies/blob/master/README.md). Finally, each page was manually inspected. Some region labels were corrected, several regions were fitted to the text lines. The latter process was supported by script provided by the OCR-D projects, see [https://github.com/OCR-D/ocrd_segment/blob/08f169c2295c7fc5ed15f12232ef742f3f5e58dd/ocrd_segment/repair.py#L44](https://github.com/OCR-D/ocrd_segment/blob/08f169c2295c7fc5ed15f12232ef742f3f5e58dd/ocrd_segment/repair.py#L44). 

## Annotations
Not applicable.

### Annotation Process
Not applicable.

### Annotators
Not applicable.

### Crowd Labour
Not applicable.

## Data Provenance
Not applicable.

## Use of Linked Open Data, Controlled Vocabulary, Multilingual Ontologies/Taxonomies
Not applicable.

## Version Information
This is the first version of the dataset, which was finalised in October 2025.

### Release Date
2025-10-31

### Date of Modification
Not applicable.

### Checksums
**MD5 and SHA256 hashes of the file OCR-D-GT-VD-SBB.zip:** 

MD5: 757b1fb86979b97847ef86795f06d660

SHA256: 19490ee8b2f945d44bbbca053f931c0e18678513838f486184e4a325c3223975


## Maintenance Plan


### Maintenance Level
*Limited Maintenance* – The data will not be updated, but any technical issues will be addressed during the lifetime of the OCR-D project, in the context of which this dataset was established.

### Update Periodicity 
Not applicable. 


# Examples and Considerations for Using the Data
The dataset is suitable to serve the task of optical character recognition as described above.

## Ethical Considerations

### Personal and Other Sensitive Information
The dataset does not contain personal or sensitive information. Since the youngest titles contained in this dataset were printed in the early 19th century, the dataset does not contain any sensitive data in the sense of contemporary privacy laws.

### Discussion of Biases
Not applicable.

### Potential Societal Impact of Using the Dataset
This dataset describes historical titles (monographs, broadsheets, dissertations) that have been published between 1509 and 1827. Most probably, the social impact of the dataset is therefore negiglible.        	

## Examples of Datasets, Publications and Models that (re-)use the Dataset 
So far, this dataset has not yet been reused.
## Known Non-Ethical Limitations
Not applicable.

## Unanticipated Uses made of this Dataset
There are no known unanticipated uses made of this dataset. Users are invited to report the uses they made of this dataset back to the curators, which would enable an update of this datasheet.

Datasheet as of October 31st, 2025


